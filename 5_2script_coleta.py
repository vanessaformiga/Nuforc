# -*- coding: utf-8 -*-
"""5.2script_coleta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yLgq_ma9Hu9c_vs2b9DoRu-onogjtnJI

# #Script da Coleta
"""

# Vamos começar a parte prática da coleta: desenvolver um script Python para executar a coleta de dados dos vinte anos, entre setembro 1997 e setembro de 2017. 
#Coloque tudo em um DataFrame e depois salve em um arquivo .csv.

# Sugestões de bibliotecas para essa etapa:
#-requests: biblioteca para execução de requisições HTTP;
#-BeautifulSoup: biblioteca para extração de dados em arquivos HTML e XML;
#-Pandas: biblioteca para armazenar, limpar e salvar os dados em forma de tabela.

# Notas importantes sobre a raspagem da web (web scraping):
# Leia os Termos e Condições do website para entender como você pode usar os dados legalmente. A maioria 
#dos sites proíbe você de usar os dados para fins comerciais.
# Verifique se você não está baixando dados a uma taxa muito rápida porque isso pode quebrar o site. Você também pode ser bloqueado do site.

from bs4 import BeautifulSoup
import requests
import time
import pandas as pd

lista = []

meses = ['01','02','03','04','05','06','07','08','09','10','11','12']

for ano in range (1997, 2018):
  for mes in meses:
    link = "{}{}.html".format('http://www.nuforc.org/webreports/ndxe',str(ano)+mes)
    
    if ano == 1997:
      if mes not in ['9', '10','11','12']:
        continue
        
    if ano == 2017:
      if mes in ['9','10','11','12']:
        continue
    
    request = requests.get(link).text
    soup = BeautifulSoup(request,'html.parser')
    tabela = soup.find_all(name="table")
    tabela_str = str(tabela)
    df = pd.read_html(tabela_str)[0]
    lista.append(df)
    time.sleep(1)

arquivo = pd.concat(lista, ignore_index = True)  
arquivo.to_csv('script_coleta.csv')
    
    #print(link)
       
#for i in range(199712, 199802):
 # numero = str(i)
  #numero[-1] + numero[-2]
  #if not dado == '00' and int(dado) <= 12:
   # request = requests.get('http://www.nuforc.org/webreports/ndxe'+numero+'.html').text
    #soup = BeautifulSoup(request,'html.parser')
    #tabela = soup.find_all(name="table")
    #tabela_str = str(tabela)
    #df = pd.read_html(tabela_str)[0]
    #lista.append(df)
    #time.sleep(1)

#arquivo = pd.concat(lista, ignore_index = True)  
#arquivo.to_csv('coleta.csv')'''